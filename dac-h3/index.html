---
layout: default
---

<div class="home">

    <header class="dummy">
        <div class="container">
            <div class="intro-text">
            </div>          
        </div>
    </header>

<section>
  <div class="container">
           <div class="row">
                <div class="col-lg-12 text-center">
                    <h6 class="section-heading"></h6>
                </div>
            </div>    
    <div class="row">
        <div class="col-lg-12">

<h1> Human-Robot Interaction with the DAC-H3 Cognitive architecture. </h1>
<!-- <h2>Note for the reviewers</h2>
<p class="large"> The videos related to our paper submitted to IEEE TCDS are currently being edited and will be available asap. Preliminary videos are available below. </p> -->
<h2>Videos</h2>

<p class="large">We present below three demonstrations of the DAC-h3 cognitive architecture. They show how the system adapts to the human behavior and that it can be executed on various environments (different robot, lab and human partner). </p>

<h3> Demonstration 1 </h3>

<p class="large"> The robot is self-regulating two drives for knowledge acquisition and expression. Acquired information is about labeling the perceived objects and associating body part touch and motor information. In addition, a number of goal-oriented behaviors are executed through human requests: taking an object, showing the learned kinematic structure, expressing a narrative. The goal for taking an object is executed twice, showing how the action plan execution adapts to the initial state of the object. </p>

<p class="large">The inset in the top right part of the screen displays information about the current state of the robot. From left to right:
<!--     <ul>
        <li>From left to right.</li> -->
        <ul>
            <li>First row: Third person view of the iCub with detected objects and human body parts; view from the iCub's left eye camera indicating the detected objects and their current associated linguistic label; perception of the human skeleton tracked by the Kinect; learned kinematic structure (only appears when requested by the human).</li>
            <li>Second row: Drive dynamics (see Figure 6 of the paper for explanation), recognition score of the most salient object, tactile sensations of the iCub's right hand.</li>
        </ul>
<!--     </ul> -->
</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/zd5HGW8s318?rel=0" frameborder="0" allowfullscreen></iframe>

<h3> Demonstration 2</h3>

<p class="large"> The robot is self-regulating two drives for knowledge acquisition and expression. Acquired information is about labeling the perceived objects, agents an body parts, as well as associating body part touch and motor information. In addition, a number goal-oriented behavior is executed through human requests for taking and giving an object. 
</p>

<p class="large">The inset in the top right part of the screen displays information about the current state of the robot.  From left to right:
        <ul>
            <li>First row: Third person view of the iCub with detected objects and human body parts (actually not moving in this video due to a minor technical issue); view from the iCub's left eye camera indicating the detected objects and their current associated linguistic label; perception of the human skeleton tracked by the Kinect. </li>
            <li>Second row: Drive dynamics (see Figure 6 of the paper for explanation), recognition score of the most salient object.</li>
        </ul>
</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Gw_6AIaBoHs?rel=0" frameborder="0" allowfullscreen></iframe>

<h3>Demonstration 3</h3>


<p class="large"> The robot is self-regulating two drives for knowledge acquisition. Acquired information is about labeling the perceived objects, agents an body parts, as well as associating body part touch and motor information. In addition, a number of goal-oriented behaviors are executed through human requests: showing the learned kinematic structure, expressing a narrative, recognizing an action, and playing with a ball. 
</p>

<p class="large">
    Compared to the two previous demonstrations above, this video was recorded with another iCub robot, in a another lab and with another interacting human, demonstrating the robustness of the system to varying conditions.
</p>


<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/8A48EgPByAA" frameborder="0" allowfullscreen></iframe>
</p>

        </div>
    </div>
  </div>
</section>
</div>  